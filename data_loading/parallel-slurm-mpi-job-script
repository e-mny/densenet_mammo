#!/bin/bash
# Usage: sbatch slurm-parallel-job-script
# Prepared By: Kai Xi,  Apr 2015
#              help@massive.org.au

# NOTE: To activate a SLURM option, remove the whitespace between the '#' and 'SBATCH'

# To give your job a name, replace "MyJob" with an appropriate name
#SBATCH --job-name=Pickling-Parallel


# To set a project account for credit charging, 
#SBATCH --account=sq58


# Request CPU resource for a parallel job, for example:  
#   4 Nodes each with 12 Cores/MPI processes
#SBATCH --ntasks=100
# SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1

# Memory usage (MB)
#SBATCH --mem-per-cpu=20000

# Set your minimum acceptable walltime, format: day-hours:minutes:seconds
#SBATCH --time=0-10:00:00


# To receive an email when job completes or fails
#SBATCH --mail-user=enoch.mok@monash.edu
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL


# Set the file for output (stdout)
#SBATCH --output=PicklingParallel-%j.out

# Set the file for error log (stderr)
#SBATCH --error=PicklingParallel-%j.err


# Use reserved node to run job when a node reservation is made for you already
# SBATCH --reservation=reservation_name


# Command to run a MPI job, 
# 
# Option 1: 'srun' is a wrapper of 'mpirun' and it can automatically detect how many MPI processes to be launched 
set -e
eval "$(conda shell.bash hook)"
conda activate exp1
srun --cpu_bind=thread python test_parallel.py

# Option 2: mpiexec
# For some cases, 'srun' does not perform the running behavior you want, you can still use raw MPI commands such as mpiexec, mpirun
# mpiexec ./you_program


# # If you want to enable bind-to-core option.
# srun --cpu_bind=cores,v ./you_program
# # Or
# mpiexec --bind-to core --report-bindings ./you_program




